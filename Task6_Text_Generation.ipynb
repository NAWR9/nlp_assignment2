{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fe9cd7",
   "metadata": {},
   "source": [
    "# Task 6 – Text Generation with an LSTM Language Model on LABR\n",
    "\n",
    "This notebook trains a word-level language model on the LABR Arabic book reviews and uses it to generate new Arabic text. The model is trained to predict the next word given a sequence of previous words (n-gram prefixes). At inference time, a seed sentence is provided and the model repeatedly predicts the next word, producing synthetic Arabic review text.\n",
    "\n",
    "A separate tokenizer and LSTM architecture are used for the generation task. The corpus size, vocabulary size, and model capacity are limited to keep memory usage manageable while still capturing basic patterns in the review language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38267f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a3b9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63257,\n",
       "    rating  review_id   user_id   book_id  \\\n",
       " 0       4  338670838   7878381  13431841   \n",
       " 1       4   39428407   1775679   3554772   \n",
       " 2       4   32159373   1304410   3554772   \n",
       " 3       1  442326656  11333112   3554772   \n",
       " 4       5   46492258    580165   3554772   \n",
       " \n",
       "                                                 text  \n",
       " 0   \"عزازيل الذي صنعناه ،الكامن في أنفسنا\" يذكرني...  \n",
       " 1   من أمتع ما قرأت من روايات بلا شك. وحول الشك ت...  \n",
       " 2   رواية تتخذ من التاريخ ،جوًا لها اختار المؤلف ...  \n",
       " 3   إني أقدّر هذه الرواية كثيرا، لسبب مختلف عن أس...  \n",
       " 4   الكاهن الذي أطلق على نفسه اسم هيبا تيمنا بالع...  )"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data/reviews.tsv\"  # adjust if necessary\n",
    "\n",
    "df = pd.read_csv(\n",
    "    data_path,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"rating\", \"review_id\", \"user_id\", \"book_id\", \"text\"],\n",
    ")\n",
    "\n",
    "len(df), df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c7a3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' \"عزازيل الذي صنعناه ،الكامن في أنفسنا\" يذكرني يوسف زيدان بــ بورخس في استخدامه لحيلته الفنية،وخداع القاريء بأن الرواية ترجمة لمخطوط قديم. الهوامش المخترعة و اختلاق وجود مترجـِم عاد بي إلى بورخس و هوامشه و كتَّابه الوهميين. هذه أولى قراءاتي ليوسف زيدان ،وهو عبقري في السرد ويخلقُ جوَّا ساحرا متفرداً يغرقك في المتعة. هُنا يتجلى الشكُّ الراقي الممزوج بانسانية هيبا الفاتنة ربما تم تناول فكرة الرواية قبلاً ،ولكن هنا تفرداً و عذوبة لا تُقارن بنصٍ آخر كنتُ أودُّ لو صيغت النهاية بطريقة مختلفة فقد جاءت باردة لا تتناسب مع رواية خُطَّت بهذا الشغف . ولذا لا أستطيع منح الرواية خمس نجوم ،وإن كانت تجربة قرائية متفردة وممتعة. ',\n",
       " '\"عزازيل الذي صنعناه ،الكامن في أنفسنا\" يذكرني يوسف زيدان ب بورخس في استخدامه لحيلته الفنية،وخداع القاريء بأن الرواية ترجمة لمخطوط قديم. الهوامش المخترعة و اختلاق وجود مترجم عاد بي إلى بورخس و هوامشه و كتابه الوهميين. هذه أولى قراءاتي ليوسف زيدان ،وهو عبقري في السرد ويخلق جوا ساحرا متفردا يغرقك في المتعة. هنا يتجلى الشك الراقي الممزوج بانسانية هيبا الفاتنة ربما تم تناول فكرة الرواية قبلا ،ولكن هنا تفردا و عذوبة لا تقارن بنص آخر كنت أود لو صيغت النهاية بطريقة مختلفة فقد جاءت باردة لا تتناسب مع رواية خطت بهذا الشغف . ولذا لا أستطيع منح الرواية خمس نجوم ،وإن كانت تجربة قرائية متفردة وممتعة.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_diacritics = re.compile(r\"[\\u0617-\\u061A\\u064B-\\u0652\\u0670\\u0640]+\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(arabic_diacritics, \"\", str(text))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "sample_raw = df.loc[0, \"text\"]\n",
    "sample_clean = clean_text(sample_raw)\n",
    "sample_raw, sample_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2b7f34",
   "metadata": {},
   "source": [
    "## Preprocessing and corpus selection\n",
    "\n",
    "The LABR dataset consists of Arabic book reviews with ratings and metadata. For text generation, only the review text is used. A simple normalization step is applied to remove Arabic diacritics and elongation marks and to trim extra whitespace, while leaving the rest of the text unchanged.\n",
    "\n",
    "To keep memory usage under control, the generation model is trained on a subset of the reviews and a limited vocabulary. A new tokenizer is fitted on the cleaned review texts, restricted to the most frequent words. N-gram sequences are then constructed from each review, where each sequence contains a prefix of words and the task is to predict the next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5161583c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in generation vocabulary: 15000\n",
      "Number of n-gram sequences: 246785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((246785, 39), (246785,), 40)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the number of reviews used for language modelling\n",
    "max_texts_for_generation = 10000\n",
    "corpus_texts = df[\"text\"].astype(str).tolist()[:max_texts_for_generation]\n",
    "\n",
    "corpus_clean = [clean_text(t) for t in corpus_texts]\n",
    "\n",
    "# Limit vocabulary size to reduce model size and memory usage\n",
    "max_vocab_gen = 15000\n",
    "gen_tokenizer = Tokenizer(num_words=max_vocab_gen, oov_token=\"<OOV>\")\n",
    "gen_tokenizer.fit_on_texts(corpus_clean)\n",
    "\n",
    "total_words = min(max_vocab_gen, len(gen_tokenizer.word_index) + 1)\n",
    "print(\"Total words in generation vocabulary:\", total_words)\n",
    "\n",
    "# Build n-gram input sequences from each cleaned review, with max length per review\n",
    "input_sequences = []\n",
    "max_tokens_per_review = 40\n",
    "\n",
    "for line in corpus_clean:\n",
    "    token_list = gen_tokenizer.texts_to_sequences([line])[0]\n",
    "    if not token_list:\n",
    "        continue\n",
    "    token_list = token_list[:max_tokens_per_review]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[: i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "print(\"Number of n-gram sequences:\", len(input_sequences))\n",
    "\n",
    "# Pad sequences so they all have the same length (pre-padding)\n",
    "max_sequence_len = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = np.array(\n",
    "    pad_sequences(input_sequences, maxlen=max_sequence_len, padding=\"pre\")\n",
    ")\n",
    "\n",
    "# Predictors and integer labels (last word is the label)\n",
    "xs = input_sequences[:, :-1]\n",
    "labels = input_sequences[:, -1].astype(\"int32\")\n",
    "\n",
    "xs.shape, labels.shape, max_sequence_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93066bee",
   "metadata": {},
   "source": [
    "## N-gram sequences and training labels\n",
    "\n",
    "Each cleaned review is tokenized into a sequence of word indices and truncated to a maximum of 40 tokens for the language modelling task. For each review, all possible n-grams are generated: for a sequence of length *L*, the prefixes of length 2, 3, …, *L* are collected as training examples. This produces a large set of sequences where the last word acts as the target and the preceding words form the input.\n",
    "\n",
    "All n-gram sequences are pre-padded to the same maximum length using `pad_sequences`, so that the model always receives inputs of fixed size. The predictors `xs` contain all tokens except the last one, while the label vector `labels` contains the corresponding next-word indices. The labels are kept as integers and used with the sparse categorical cross-entropy loss to avoid constructing very large one-hot matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7587f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 39, 32)            480000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 15000)             495000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,001,912\n",
      "Trainable params: 1,001,912\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim_gen = 32\n",
    "lstm_units_gen = 64\n",
    "\n",
    "gen_model = models.Sequential([\n",
    "    layers.Embedding(total_words, embedding_dim_gen, input_length=max_sequence_len - 1),\n",
    "    layers.LSTM(lstm_units_gen),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(total_words, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "optimizer_gen = optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "gen_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=optimizer_gen,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "gen_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0444471",
   "metadata": {},
   "source": [
    "## LSTM language model architecture\n",
    "\n",
    "The language model is implemented as a word-level LSTM network. An embedding layer maps each token index to a dense vector of size 32, which serves as the input to an LSTM layer with 64 hidden units. The LSTM processes the sequence of embeddings and encodes the prefix into a single hidden state. Dropout and recurrent dropout with a rate of 0.3 are applied inside the LSTM to reduce overfitting.\n",
    "\n",
    "The final hidden state is passed through a dense layer with 32 ReLU units, followed by a softmax output layer over the vocabulary. The network is trained with the Adam optimizer and sparse categorical cross-entropy loss, treating next-word prediction as a multi-class classification problem over all words in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b1728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1929/1929 - 399s - loss: 7.2096 - accuracy: 0.1420 - 399s/epoch - 207ms/step\n",
      "Epoch 2/15\n",
      "1929/1929 - 386s - loss: 6.9145 - accuracy: 0.1467 - 386s/epoch - 200ms/step\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m      2\u001b[0m batch_size_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m----> 4\u001b[0m history_gen \u001b[38;5;241m=\u001b[39m \u001b[43mgen_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs_gen = 15\n",
    "batch_size_gen = 128\n",
    "\n",
    "history_gen = gen_model.fit(\n",
    "    xs,\n",
    "    labels,\n",
    "    epochs=num_epochs_gen,\n",
    "    batch_size=batch_size_gen,\n",
    "    verbose=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gen_metric(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.show()\n",
    "\n",
    "plot_gen_metric(history_gen, \"accuracy\")\n",
    "plot_gen_metric(history_gen, \"loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb374ee",
   "metadata": {},
   "source": [
    "## Training behaviour\n",
    "\n",
    "The accuracy and loss curves summarize how the LSTM language model fits the n-gram data over the training epochs. A steady increase in training accuracy together with a decrease in loss indicates that the model is learning useful associations between prefixes and next words. Because the task is trained only on next-word prediction without an explicit validation split, the curves reflect the fit on the training set. The relatively smooth trend suggests that the chosen model capacity and regularization are sufficient for capturing common patterns in the review corpus without immediately overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca924c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse mapping from index to word for generation\n",
    "reverse_word_index_gen = {idx: word for word, idx in gen_tokenizer.word_index.items()}\n",
    "\n",
    "def generate_text(seed_text, next_words=20):\n",
    "    \"\"\"\n",
    "    Generate text by repeatedly predicting the next word given the current sequence.\n",
    "    \"\"\"\n",
    "    text = clean_text(seed_text)\n",
    "    for _ in range(next_words):\n",
    "        token_list = gen_tokenizer.texts_to_sequences([text])[0]\n",
    "        if not token_list:\n",
    "            break\n",
    "        token_list = pad_sequences(\n",
    "            [token_list],\n",
    "            maxlen=max_sequence_len - 1,\n",
    "            padding=\"pre\",\n",
    "        )\n",
    "        predicted_probs = gen_model.predict(token_list, verbose=0)[0]\n",
    "        predicted_index = int(np.argmax(predicted_probs))\n",
    "        if predicted_index == 0:\n",
    "            break\n",
    "        next_word = reverse_word_index_gen.get(predicted_index, \"\")\n",
    "        if not next_word or next_word == \"<OOV>\":\n",
    "            break\n",
    "        text += \" \" + next_word\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc4865",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_sentences = [\n",
    "    \"هذا الكتاب\",\n",
    "    \"القصة كانت\",\n",
    "    \"أعتقد أن الكاتب\",\n",
    "]\n",
    "\n",
    "for seed in seed_sentences:\n",
    "    generated = generate_text(seed, next_words=25)\n",
    "    print(\"Seed:\", seed)\n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36104fc6",
   "metadata": {},
   "source": [
    "## Analysis of generated Arabic text\n",
    "\n",
    "The language model is trained to predict the next word from a sequence of preceding words extracted from Arabic book reviews. When given short seed phrases such as \"هذا الكتاب\"، \"القصة كانت\"، and \"أعتقد أن الكاتب\"، the model generates continuations that often resemble review-like language, using frequent expressions and collocations learned from the corpus. In many cases, the local word combinations are plausible and reflect common patterns in how readers describe books, stories, and authors.\n",
    "\n",
    "However, the generated sequences are not always fully grammatical or semantically coherent. Over longer spans, the text may contain repetitions, abrupt topic shifts, or awkward phrasing. These limitations are expected, since the model is trained only with next-word prediction and uses a relatively small architecture and truncated context window. Overall, the stacked LSTM demonstrates the ability to capture some stylistic and lexical regularities of Arabic book reviews and to produce short, review-like fragments, while also illustrating the challenges of generating fluent, human-like Arabic text using a basic recurrent language model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
