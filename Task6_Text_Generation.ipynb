{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fe9cd7",
   "metadata": {},
   "source": [
    "# Task 6 – Text Generation with an LSTM Language Model on LABR\n",
    "\n",
    "This notebook trains a word-level language model on the LABR Arabic book reviews and uses it to generate new Arabic text. The model is trained to predict the next word given a sequence of previous words (n-gram prefixes). At inference time, a seed sentence is provided and the model repeatedly predicts the next word, producing synthetic Arabic review text.\n",
    "\n",
    "A separate tokenizer and LSTM architecture are used for the generation task. The corpus size, vocabulary size, and model capacity are limited to keep memory usage manageable while still capturing basic patterns in the review language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38267f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a3b9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63257,\n",
       "    rating  review_id   user_id   book_id  \\\n",
       " 0       4  338670838   7878381  13431841   \n",
       " 1       4   39428407   1775679   3554772   \n",
       " 2       4   32159373   1304410   3554772   \n",
       " 3       1  442326656  11333112   3554772   \n",
       " 4       5   46492258    580165   3554772   \n",
       " \n",
       "                                                 text  \n",
       " 0   \"عزازيل الذي صنعناه ،الكامن في أنفسنا\" يذكرني...  \n",
       " 1   من أمتع ما قرأت من روايات بلا شك. وحول الشك ت...  \n",
       " 2   رواية تتخذ من التاريخ ،جوًا لها اختار المؤلف ...  \n",
       " 3   إني أقدّر هذه الرواية كثيرا، لسبب مختلف عن أس...  \n",
       " 4   الكاهن الذي أطلق على نفسه اسم هيبا تيمنا بالع...  )"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data/reviews.tsv\"  # adjust if necessary\n",
    "\n",
    "df = pd.read_csv(\n",
    "    data_path,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"rating\", \"review_id\", \"user_id\", \"book_id\", \"text\"],\n",
    ")\n",
    "\n",
    "len(df), df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c7a3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' \"عزازيل الذي صنعناه ،الكامن في أنفسنا\" يذكرني يوسف زيدان بــ بورخس في استخدامه لحيلته الفنية،وخداع القاريء بأن الرواية ترجمة لمخطوط قديم. الهوامش المخترعة و اختلاق وجود مترجـِم عاد بي إلى بورخس و هوامشه و كتَّابه الوهميين. هذه أولى قراءاتي ليوسف زيدان ،وهو عبقري في السرد ويخلقُ جوَّا ساحرا متفرداً يغرقك في المتعة. هُنا يتجلى الشكُّ الراقي الممزوج بانسانية هيبا الفاتنة ربما تم تناول فكرة الرواية قبلاً ،ولكن هنا تفرداً و عذوبة لا تُقارن بنصٍ آخر كنتُ أودُّ لو صيغت النهاية بطريقة مختلفة فقد جاءت باردة لا تتناسب مع رواية خُطَّت بهذا الشغف . ولذا لا أستطيع منح الرواية خمس نجوم ،وإن كانت تجربة قرائية متفردة وممتعة. ',\n",
       " '\"عزازيل الذي صنعناه ،الكامن في أنفسنا\" يذكرني يوسف زيدان ب بورخس في استخدامه لحيلته الفنية،وخداع القاريء بأن الرواية ترجمة لمخطوط قديم. الهوامش المخترعة و اختلاق وجود مترجم عاد بي إلى بورخس و هوامشه و كتابه الوهميين. هذه أولى قراءاتي ليوسف زيدان ،وهو عبقري في السرد ويخلق جوا ساحرا متفردا يغرقك في المتعة. هنا يتجلى الشك الراقي الممزوج بانسانية هيبا الفاتنة ربما تم تناول فكرة الرواية قبلا ،ولكن هنا تفردا و عذوبة لا تقارن بنص آخر كنت أود لو صيغت النهاية بطريقة مختلفة فقد جاءت باردة لا تتناسب مع رواية خطت بهذا الشغف . ولذا لا أستطيع منح الرواية خمس نجوم ،وإن كانت تجربة قرائية متفردة وممتعة.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_diacritics = re.compile(r\"[\\u0617-\\u061A\\u064B-\\u0652\\u0670\\u0640]+\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(arabic_diacritics, \"\", str(text))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "sample_raw = df.loc[0, \"text\"]\n",
    "sample_clean = clean_text(sample_raw)\n",
    "sample_raw, sample_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2b7f34",
   "metadata": {},
   "source": [
    "## Preprocessing and corpus selection\n",
    "\n",
    "The LABR dataset consists of Arabic book reviews with ratings and metadata. For text generation, only the review text is used. A simple normalization step is applied to remove Arabic diacritics and elongation marks and to trim extra whitespace, while leaving the rest of the text unchanged.\n",
    "\n",
    "To keep memory usage under control, the generation model is trained on a subset of the reviews and a limited vocabulary. A new tokenizer is fitted on the cleaned review texts, restricted to the most frequent words. N-gram sequences are then constructed from each review, where each sequence contains a prefix of words and the task is to predict the next word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5161583c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in generation vocabulary: 15000\n",
      "Number of n-gram sequences: 246785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((246785, 39), (246785,), 40)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the number of reviews used for language modelling\n",
    "max_texts_for_generation = 10000\n",
    "corpus_texts = df[\"text\"].astype(str).tolist()[:max_texts_for_generation]\n",
    "\n",
    "corpus_clean = [clean_text(t) for t in corpus_texts]\n",
    "\n",
    "# Limit vocabulary size to reduce model size and memory usage\n",
    "max_vocab_gen = 15000\n",
    "gen_tokenizer = Tokenizer(num_words=max_vocab_gen, oov_token=\"<OOV>\")\n",
    "gen_tokenizer.fit_on_texts(corpus_clean)\n",
    "\n",
    "total_words = min(max_vocab_gen, len(gen_tokenizer.word_index) + 1)\n",
    "print(\"Total words in generation vocabulary:\", total_words)\n",
    "\n",
    "# Build n-gram input sequences from each cleaned review, with max length per review\n",
    "input_sequences = []\n",
    "max_tokens_per_review = 40\n",
    "\n",
    "for line in corpus_clean:\n",
    "    token_list = gen_tokenizer.texts_to_sequences([line])[0]\n",
    "    if not token_list:\n",
    "        continue\n",
    "    token_list = token_list[:max_tokens_per_review]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[: i + 1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "print(\"Number of n-gram sequences:\", len(input_sequences))\n",
    "\n",
    "# Pad sequences so they all have the same length (pre-padding)\n",
    "max_sequence_len = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = np.array(\n",
    "    pad_sequences(input_sequences, maxlen=max_sequence_len, padding=\"pre\")\n",
    ")\n",
    "\n",
    "# Predictors and integer labels (last word is the label)\n",
    "xs = input_sequences[:, :-1]\n",
    "labels = input_sequences[:, -1].astype(\"int32\")\n",
    "\n",
    "xs.shape, labels.shape, max_sequence_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93066bee",
   "metadata": {},
   "source": [
    "## N-gram sequences and training labels\n",
    "\n",
    "Each cleaned review is tokenized into a sequence of word indices and truncated to a maximum of 40 tokens for the language modelling task. For each review, all possible n-grams are generated: for a sequence of length *L*, the prefixes of length 2, 3, …, *L* are collected as training examples. This produces a large set of sequences where the last word acts as the target and the preceding words form the input.\n",
    "\n",
    "All n-gram sequences are pre-padded to the same maximum length using `pad_sequences`, so that the model always receives inputs of fixed size. The predictors `xs` contain all tokens except the last one, while the label vector `labels` contains the corresponding next-word indices. The labels are kept as integers and used with the sparse categorical cross-entropy loss to avoid constructing very large one-hot matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7587f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 39, 32)            480000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 15000)             495000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,001,912\n",
      "Trainable params: 1,001,912\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Improved model hyperparameters\n",
    "embedding_dim_gen = 128  # Increased from 32 for richer word representations\n",
    "lstm_units_gen = 256     # Increased from 64 for more model capacity\n",
    "dropout_rate = 0.3       # Regularization to prevent overfitting\n",
    "\n",
    "gen_model = models.Sequential([\n",
    "    # Embedding layer with larger dimension for better word representations\n",
    "    layers.Embedding(total_words, embedding_dim_gen, input_length=max_sequence_len - 1),\n",
    "    \n",
    "    # First LSTM layer (returns sequences for stacking)\n",
    "    layers.LSTM(lstm_units_gen, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate),\n",
    "    \n",
    "    # Second LSTM layer for deeper feature extraction\n",
    "    layers.LSTM(lstm_units_gen, dropout=dropout_rate, recurrent_dropout=dropout_rate),\n",
    "    \n",
    "    # Dense layers with batch normalization and dropout for regularization\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dropout(dropout_rate),\n",
    "    layers.Dense(total_words, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "optimizer_gen = optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "gen_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=optimizer_gen,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "gen_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0444471",
   "metadata": {},
   "source": [
    "## Improved LSTM language model architecture\n",
    "\n",
    "The language model is implemented as a stacked word-level LSTM network with several improvements for better text generation:\n",
    "\n",
    "1. **Larger Embedding Dimension (128)**: The embedding layer maps each token index to a dense vector of size 128 (increased from 32), allowing richer semantic representations of words.\n",
    "\n",
    "2. **Stacked LSTM Layers (2 × 256 units)**: Two LSTM layers are stacked, each with 256 hidden units. The first layer returns sequences to feed into the second, enabling the model to learn hierarchical temporal patterns. This deeper architecture captures more complex dependencies in the text.\n",
    "\n",
    "3. **Dropout Regularization (0.3)**: Both dropout and recurrent dropout with a rate of 0.3 are applied in each LSTM layer to prevent overfitting and improve generalization.\n",
    "\n",
    "4. **Batch Normalization**: Added before the dense layers to stabilize training and allow faster convergence.\n",
    "\n",
    "5. **Larger Dense Layer (256 units)**: The final hidden state passes through a dense layer with 256 ReLU units, followed by dropout and a softmax output layer over the vocabulary.\n",
    "\n",
    "The network is trained with the Adam optimizer and sparse categorical cross-entropy loss, treating next-word prediction as a multi-class classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b1728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1929/1929 - 399s - loss: 7.2096 - accuracy: 0.1420 - 399s/epoch - 207ms/step\n",
      "Epoch 2/15\n",
      "1929/1929 - 386s - loss: 6.9145 - accuracy: 0.1467 - 386s/epoch - 200ms/step\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m      2\u001b[0m batch_size_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m----> 4\u001b[0m history_gen \u001b[38;5;241m=\u001b[39m \u001b[43mgen_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\osama\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "num_epochs_gen = 30  # Increased epochs (early stopping will prevent overfitting)\n",
    "batch_size_gen = 128\n",
    "\n",
    "# Callbacks for better training\n",
    "callbacks = [\n",
    "    # Stop training when validation loss stops improving\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Reduce learning rate when validation loss plateaus\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "history_gen = gen_model.fit(\n",
    "    xs,\n",
    "    labels,\n",
    "    epochs=num_epochs_gen,\n",
    "    batch_size=batch_size_gen,\n",
    "    validation_split=0.1,  # Use 10% of data for validation\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gen_metric(history, metric):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history[metric], label=f\"Training {metric}\")\n",
    "    if f\"val_{metric}\" in history.history:\n",
    "        plt.plot(history.history[f\"val_{metric}\"], label=f\"Validation {metric}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.title(f\"Training and Validation {metric.capitalize()}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_gen_metric(history_gen, \"accuracy\")\n",
    "plot_gen_metric(history_gen, \"loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb374ee",
   "metadata": {},
   "source": [
    "## Training behaviour\n",
    "\n",
    "The accuracy and loss curves now show both training and validation metrics, providing insight into the model's generalization ability. Key improvements in the training process include:\n",
    "\n",
    "1. **Validation Split (10%)**: A portion of the data is held out to monitor overfitting. The gap between training and validation curves indicates the degree of overfitting.\n",
    "\n",
    "2. **Early Stopping**: Training automatically stops when validation loss stops improving for 5 consecutive epochs, preventing overfitting and saving computation time.\n",
    "\n",
    "3. **Learning Rate Reduction**: The learning rate is automatically reduced by half when validation loss plateaus, allowing finer convergence in later training stages.\n",
    "\n",
    "A steady increase in training accuracy together with decreasing loss indicates that the model is learning useful associations between prefixes and next words. The validation curves help identify when the model starts to memorize rather than generalize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca924c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse mapping from index to word for generation\n",
    "reverse_word_index_gen = {idx: word for word, idx in gen_tokenizer.word_index.items()}\n",
    "\n",
    "def sample_with_temperature(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample from the probability distribution with temperature scaling.\n",
    "    \n",
    "    - temperature < 1.0: More deterministic (sharper distribution)\n",
    "    - temperature = 1.0: Sample from the original distribution\n",
    "    - temperature > 1.0: More random (flatter distribution)\n",
    "    \"\"\"\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions + 1e-10) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def top_k_sampling(predictions, k=10):\n",
    "    \"\"\"\n",
    "    Sample from the top-k most probable words.\n",
    "    This prevents sampling very unlikely tokens while maintaining diversity.\n",
    "    \"\"\"\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    # Get indices of top k predictions\n",
    "    top_k_indices = np.argsort(predictions)[-k:]\n",
    "    # Zero out all other probabilities\n",
    "    top_k_probs = np.zeros_like(predictions)\n",
    "    top_k_probs[top_k_indices] = predictions[top_k_indices]\n",
    "    # Renormalize\n",
    "    top_k_probs = top_k_probs / np.sum(top_k_probs)\n",
    "    # Sample from the top-k distribution\n",
    "    return np.random.choice(len(predictions), p=top_k_probs)\n",
    "\n",
    "def generate_text(seed_text, next_words=20, temperature=0.8, top_k=None, greedy=False):\n",
    "    \"\"\"\n",
    "    Generate text by repeatedly predicting the next word given the current sequence.\n",
    "    \n",
    "    Args:\n",
    "        seed_text: Starting text for generation\n",
    "        next_words: Number of words to generate\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "        top_k: If set, sample only from top-k most likely words\n",
    "        greedy: If True, always pick the most likely word (argmax)\n",
    "    \"\"\"\n",
    "    text = clean_text(seed_text)\n",
    "    for _ in range(next_words):\n",
    "        token_list = gen_tokenizer.texts_to_sequences([text])[0]\n",
    "        if not token_list:\n",
    "            break\n",
    "        token_list = pad_sequences(\n",
    "            [token_list],\n",
    "            maxlen=max_sequence_len - 1,\n",
    "            padding=\"pre\",\n",
    "        )\n",
    "        predicted_probs = gen_model.predict(token_list, verbose=0)[0]\n",
    "        \n",
    "        # Choose sampling strategy\n",
    "        if greedy:\n",
    "            predicted_index = int(np.argmax(predicted_probs))\n",
    "        elif top_k is not None:\n",
    "            predicted_index = int(top_k_sampling(predicted_probs, k=top_k))\n",
    "        else:\n",
    "            predicted_index = int(sample_with_temperature(predicted_probs, temperature))\n",
    "        \n",
    "        if predicted_index == 0:\n",
    "            break\n",
    "        next_word = reverse_word_index_gen.get(predicted_index, \"\")\n",
    "        if not next_word or next_word == \"<OOV>\":\n",
    "            break\n",
    "        text += \" \" + next_word\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc4865",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_sentences = [\n",
    "    \"هذا الكتاب\",\n",
    "    \"القصة كانت\",\n",
    "    \"أعتقد أن الكاتب\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GREEDY SAMPLING (deterministic)\")\n",
    "print(\"=\" * 80)\n",
    "for seed in seed_sentences:\n",
    "    generated = generate_text(seed, next_words=25, greedy=True)\n",
    "    print(\"Seed:\", seed)\n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEMPERATURE SAMPLING (temperature=0.7)\")\n",
    "print(\"=\" * 80)\n",
    "for seed in seed_sentences:\n",
    "    generated = generate_text(seed, next_words=25, temperature=0.7)\n",
    "    print(\"Seed:\", seed)\n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP-K SAMPLING (k=10)\")\n",
    "print(\"=\" * 80)\n",
    "for seed in seed_sentences:\n",
    "    generated = generate_text(seed, next_words=25, top_k=10)\n",
    "    print(\"Seed:\", seed)\n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36104fc6",
   "metadata": {},
   "source": [
    "## Analysis of generated Arabic text\n",
    "\n",
    "The improved language model demonstrates several sampling strategies for text generation:\n",
    "\n",
    "### Sampling Strategies\n",
    "\n",
    "1. **Greedy Sampling**: Always selects the most probable next word. This produces the most deterministic output but may lead to repetitive or generic text.\n",
    "\n",
    "2. **Temperature Sampling**: Adjusts the \"sharpness\" of the probability distribution:\n",
    "   - Lower temperature (e.g., 0.5): More focused on likely words, producing coherent but less diverse text\n",
    "   - Higher temperature (e.g., 1.2): More random selection, producing more creative but potentially less coherent text\n",
    "\n",
    "3. **Top-K Sampling**: Restricts selection to the K most probable words, then samples from this subset. This prevents the model from selecting very unlikely words while maintaining some diversity.\n",
    "\n",
    "### Observations\n",
    "\n",
    "The stacked LSTM with improved architecture and training produces text that:\n",
    "- Uses frequent expressions and collocations learned from Arabic book reviews\n",
    "- Shows better contextual understanding due to the deeper architecture\n",
    "- Generates more varied outputs with temperature/top-k sampling compared to greedy decoding\n",
    "\n",
    "The model still has limitations inherent to word-level language models: occasional grammatical inconsistencies, topic drift in longer sequences, and sensitivity to the sampling parameters. However, the improvements in architecture (stacked LSTMs, larger embeddings, regularization) and training (validation-based early stopping, learning rate scheduling) help produce more natural-sounding Arabic text compared to the baseline model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
